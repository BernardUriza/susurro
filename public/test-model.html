<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Model Loading Test</title>
    <style>
        body {
            font-family: monospace;
            padding: 20px;
            background: #1a1a1a;
            color: #00ff00;
        }
        pre {
            background: #000;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .error { color: #ff0000; }
        .success { color: #00ff00; }
        .info { color: #ffff00; }
    </style>
</head>
<body>
    <h1>Whisper Model Loading Test</h1>
    <div id="output"></div>
    
    <script type="module">
        const output = document.getElementById('output');
        
        function log(message, type = 'info') {
            const div = document.createElement('div');
            div.className = type;
            div.textContent = message;
            output.appendChild(div);
            console.log(message);
        }
        
        async function testModelLoading() {
            log('=== Starting Whisper Model Test ===', 'info');
            
            // Test 1: Direct file access
            log('\n1. Testing direct file access:', 'info');
            try {
                const response = await fetch('/models/whisper-tiny/config.json');
                log(`   Fetch status: ${response.status}`, response.ok ? 'success' : 'error');
                log(`   Content-Type: ${response.headers.get('content-type')}`, 'info');
                
                if (response.ok) {
                    const data = await response.json();
                    log(`   ✓ Config loaded: ${data._name_or_path}`, 'success');
                }
            } catch (error) {
                log(`   ✗ Error: ${error.message}`, 'error');
            }
            
            // Test 2: Import @xenova/transformers
            log('\n2. Loading @xenova/transformers:', 'info');
            try {
                const { pipeline, env } = await import('@xenova/transformers');
                
                // Configure for local models
                env.allowLocalModels = true;
                env.allowRemoteModels = false;
                env.localModelPath = '/models/';
                env.useBrowserCache = false;
                
                log('   Configuration:', 'info');
                log(`   - allowLocalModels: ${env.allowLocalModels}`, 'info');
                log(`   - allowRemoteModels: ${env.allowRemoteModels}`, 'info');
                log(`   - localModelPath: ${env.localModelPath}`, 'info');
                
                // Test 3: Load the model
                log('\n3. Loading Whisper model:', 'info');
                
                const transcriber = await pipeline(
                    'automatic-speech-recognition',
                    'whisper-tiny',
                    {
                        progress_callback: (progress) => {
                            if (progress.status) {
                                const percent = progress.progress ? ` (${Math.round(progress.progress * 100)}%)` : '';
                                const file = progress.file ? ` - ${progress.file}` : '';
                                log(`   ${progress.status}${file}${percent}`, 'info');
                            }
                        }
                    }
                );
                
                log('\n✓ MODEL LOADED SUCCESSFULLY!', 'success');
                return transcriber;
                
            } catch (error) {
                log(`\n✗ Error loading model: ${error.message}`, 'error');
                log(`   Stack: ${error.stack}`, 'error');
                
                // Check network tab for failed requests
                if (error.message.includes('not found locally')) {
                    log('\n   Debugging info:', 'info');
                    log('   The model files exist on the server but transformers.js cannot find them.', 'info');
                    log('   This suggests a path resolution issue.', 'info');
                }
            }
        }
        
        // Run the test
        testModelLoading().then(model => {
            if (model) {
                log('\n=== Model is ready for transcription ===', 'success');
            }
        });
    </script>
</body>
</html>