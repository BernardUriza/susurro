<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Whisper Model Loading Diagnosis</title>
    <style>
        body { font-family: monospace; padding: 20px; background: #1a1a1a; color: #0f0; }
        .error { color: #f00; font-weight: bold; }
        .success { color: #0f0; font-weight: bold; }
        .warn { color: #ff0; }
        .box { border: 1px solid #444; padding: 10px; margin: 10px 0; background: #000; }
    </style>
</head>
<body>
    <h1>Whisper Model Loading Diagnosis</h1>
    <div id="output"></div>
    
    <script type="module">
        const output = document.getElementById('output');
        
        function addSection(title, content, className = '') {
            const div = document.createElement('div');
            div.className = 'box ' + className;
            div.innerHTML = `<h3>${title}</h3><pre>${content}</pre>`;
            output.appendChild(div);
        }
        
        async function diagnose() {
            // Step 1: Check file accessibility
            addSection('Step 1: Checking Model Files', 'Testing direct access to model files...');
            
            const files = [
                '/models/whisper-tiny/config.json',
                '/models/whisper-tiny/tokenizer.json',
                '/models/whisper-tiny/preprocessor_config.json',
                '/models/whisper-tiny/onnx/encoder_model.onnx',
                '/models/whisper-tiny/onnx/decoder_model_merged.onnx'
            ];
            
            let allFilesOk = true;
            let fileResults = '';
            
            for (const file of files) {
                try {
                    const response = await fetch(file);
                    const status = response.status;
                    const contentType = response.headers.get('content-type');
                    
                    if (status === 200) {
                        fileResults += `✓ ${file}\n  Status: ${status}, Type: ${contentType}\n`;
                    } else {
                        fileResults += `✗ ${file}\n  Status: ${status}\n`;
                        allFilesOk = false;
                    }
                    
                    // Check if we're getting HTML instead of the expected file
                    if (contentType?.includes('text/html') && !file.endsWith('.html')) {
                        fileResults += `  ⚠️ WARNING: Got HTML response for non-HTML file!\n`;
                        const text = await response.text();
                        if (text.includes('<!DOCTYPE') || text.includes('<!doctype')) {
                            fileResults += `  This is likely the index.html or 404 page.\n`;
                            allFilesOk = false;
                        }
                    }
                } catch (error) {
                    fileResults += `✗ ${file}\n  Error: ${error.message}\n`;
                    allFilesOk = false;
                }
            }
            
            addSection('File Access Results', fileResults, allFilesOk ? 'success' : 'error');
            
            // Step 2: Import and configure transformers
            addSection('Step 2: Loading @xenova/transformers', 'Importing library...');
            
            try {
                const { pipeline, env } = await import('@xenova/transformers');
                
                // Log default configuration
                const defaultConfig = `Default Configuration:
allowLocalModels: ${env.allowLocalModels}
allowRemoteModels: ${env.allowRemoteModels}
localModelPath: ${env.localModelPath}
remoteURL: ${env.remoteURL}
useBrowserCache: ${env.useBrowserCache}`;
                
                addSection('Default Configuration', defaultConfig);
                
                // Apply our configuration
                env.allowLocalModels = true;
                env.allowRemoteModels = false;
                env.localModelPath = '/models/';
                env.useBrowserCache = false;
                
                const appliedConfig = `Applied Configuration:
allowLocalModels: ${env.allowLocalModels}
allowRemoteModels: ${env.allowRemoteModels}
localModelPath: ${env.localModelPath}
useBrowserCache: ${env.useBrowserCache}`;
                
                addSection('Applied Configuration', appliedConfig, 'warn');
                
                // Step 3: Attempt to load the model
                addSection('Step 3: Loading Model Pipeline', 'Attempting to create ASR pipeline...');
                
                let progressLog = '';
                const startTime = Date.now();
                
                try {
                    const transcriber = await pipeline(
                        'automatic-speech-recognition',
                        'whisper-tiny',
                        {
                            progress_callback: (progress) => {
                                const timestamp = ((Date.now() - startTime) / 1000).toFixed(2);
                                progressLog += `[${timestamp}s] ${progress.status || 'progress'}`;
                                if (progress.file) progressLog += `: ${progress.file}`;
                                if (progress.progress) progressLog += ` (${Math.round(progress.progress * 100)}%)`;
                                if (progress.loaded && progress.total) {
                                    progressLog += ` [${(progress.loaded / 1024 / 1024).toFixed(1)}MB / ${(progress.total / 1024 / 1024).toFixed(1)}MB]`;
                                }
                                progressLog += '\n';
                            }
                        }
                    );
                    
                    addSection('Loading Progress', progressLog || 'No progress events received', 'success');
                    addSection('SUCCESS!', '✓ Model loaded successfully!\n\nThe Whisper model is now ready for transcription.', 'success');
                    
                    // Make it available globally for testing
                    window.whisperModel = transcriber;
                    
                } catch (modelError) {
                    addSection('Loading Progress', progressLog || 'No progress events received', 'warn');
                    
                    // Detailed error analysis
                    let errorAnalysis = `Error Message: ${modelError.message}\n\n`;
                    
                    // Parse the error to understand what file it's looking for
                    const fileMatch = modelError.message.match(/at "([^"]+)"/);
                    if (fileMatch) {
                        errorAnalysis += `File it's looking for: ${fileMatch[1]}\n`;
                        
                        // Check if this file exists at the expected location
                        const expectedPath = fileMatch[1];
                        try {
                            const checkResponse = await fetch(expectedPath);
                            errorAnalysis += `Checking ${expectedPath}: Status ${checkResponse.status}\n`;
                            if (checkResponse.status === 404) {
                                errorAnalysis += '\nThe file path is incorrect or the file is not being served.\n';
                            }
                        } catch (e) {
                            errorAnalysis += `\nCouldn't check the path: ${e.message}\n`;
                        }
                    }
                    
                    // Check if it's a path construction issue
                    if (modelError.message.includes('not found locally')) {
                        errorAnalysis += '\nThis error indicates the library cannot find the model files.\n';
                        errorAnalysis += 'Possible causes:\n';
                        errorAnalysis += '1. The localModelPath is not being applied correctly\n';
                        errorAnalysis += '2. The model name needs to be a full URL instead of just "whisper-tiny"\n';
                        errorAnalysis += '3. The library expects a different directory structure\n';
                    }
                    
                    addSection('Error Analysis', errorAnalysis, 'error');
                    
                    // Try alternative approach
                    addSection('Step 4: Trying Alternative Approach', 'Using full URL path...');
                    
                    try {
                        // Reset and try with full URL
                        env.allowLocalModels = false;
                        env.allowRemoteModels = true;
                        
                        const transcriber2 = await pipeline(
                            'automatic-speech-recognition',
                            window.location.origin + '/models/whisper-tiny',
                            {
                                progress_callback: (progress) => {
                                    console.log('Alternative progress:', progress);
                                }
                            }
                        );
                        
                        addSection('Alternative Success!', '✓ Model loaded with full URL approach!', 'success');
                        window.whisperModel = transcriber2;
                        
                    } catch (altError) {
                        addSection('Alternative Failed', `Error: ${altError.message}`, 'error');
                    }
                }
                
            } catch (importError) {
                addSection('Import Error', `Failed to import @xenova/transformers: ${importError.message}`, 'error');
            }
        }
        
        // Run diagnosis
        diagnose();
    </script>
</body>
</html>